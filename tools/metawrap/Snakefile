#!/usr/bin/env python3

from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
from pathlib import Path

def get_file_url(wildcards):
    HTTP = HTTPRemoteProvider()
    return(
        HTTP.remote(
            read_urls[wildcards.sample],
            keep_local=True)
        )

metawrap = 'docker://quay.io/biocontainers/metawrap-mg:1.3.0--hdfd78af_0'
sra_tools = 'docker://quay.io/biocontainers/sra-tools:2.11.0--pl5321ha49a11a_3'

# metawrap tutorial test
# https://github.com/bxlab/metaWRAP/blob/master/Usage_tutorial.md
read_urls = {
    'ERR011347': 'https://sra-pub-run-odp.s3.amazonaws.com/sra/ERR011347/ERR011347',
    'ERR011348': 'https://sra-pub-run-odp.s3.amazonaws.com/sra/ERR011348/ERR011348',
    'ERR011349': 'https://sra-pub-run-odp.s3.amazonaws.com/sra/ERR011349/ERR011349',
}


all_samples = sorted(set(read_urls.keys()))

rule target:
    input:
        'test_pipeline/INITIAL_BINNING'


rule metawrap_binning:
    input:
        r1 = expand(
            'test_pipeline/READ_QC/{sample}/final_pure_reads_1.fastq',
            sample=all_samples),
        r2 = expand(
            'test_pipeline/READ_QC/{sample}/final_pure_reads_2.fastq',
            sample=all_samples),
        assembly = 'test_pipeline/ASSEMBLY/final_assembly.fasta'
    output:    
        directory('test_pipeline/INITIAL_BINNING')
    params:
        outdir = 'test_pipeline/INITIAL_BINNING'
    log:
        'test_pipeline/logs/metawrap_binning.log'
    threads:
        workflow.cores
    resources:
        mem_gb = 50
    container:
        metawrap
    shell:
        'metawrap binning '
        '--metabat2 --maxbin2 --concoct '
        '-a {input.assembly} '
        '-t {threads} '
        '-o {params.outdir} '
        '{input.r1} '
        '{input.r2} '
        '&> {log}'



rule metawrap_assembly:
    input:
        r1 = 'test_pipeline/CLEAN_READS/ALL_READS_1.fastq',
        r2 = 'test_pipeline/CLEAN_READS/ALL_READS_2.fastq'
    output:    
        'test_pipeline/ASSEMBLY/final_assembly.fasta'
    params:
        outdir = 'test_pipeline/ASSEMBLY'
    log:
        'test_pipeline/logs/metawrap_assembly.log'
    threads:
        workflow.cores
    resources:
        mem_gb = 50
    container:
        metawrap
    shell:
        'metawrap assembly '
        '--metaspades '
        '-1 {input.r1} '
        '-2 {input.r2} '
        '-t {threads} '
        '-m {resources.mem_gb} '
        '-o {params.outdir} '
        '&> {log}'


rule combine_reads:
    input:
        r1 = expand(
            'test_pipeline/READ_QC/{sample}/final_pure_reads_1.fastq',
            sample=all_samples),
        r2 = expand(
            'test_pipeline/READ_QC/{sample}/final_pure_reads_2.fastq',
            sample=all_samples)
    output:
        r1 = 'test_pipeline/CLEAN_READS/ALL_READS_1.fastq',
        r2 = 'test_pipeline/CLEAN_READS/ALL_READS_2.fastq'
    threads:
        2
    shell:
        'cat {input.r1} >> {output.r1} & '
        'cat {input.r2} >> {output.r2} & '
        'wait'


# this BMTAGGER DB just comes from NCBI's fasta
# https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz
# renamed the file to hg38.fa and built the index with the following two commands
# bmtool -d hg38.fa -o hg38.bitmask
# srprism mkindex -i hg38.fa -o hg38.srprism -M 50000
rule metawrap_readqc:
    input:
        r1 = 'test_pipeline/read_files/{sample}_1.fastq',
        r2 = 'test_pipeline/read_files/{sample}_2.fastq',
        BMTAGGER_INDEX = Path('DBs/BMTAGGER_INDEX').resolve()
    output:
        r1 = 'test_pipeline/READ_QC/{sample}/final_pure_reads_1.fastq',
        r2 = 'test_pipeline/READ_QC/{sample}/final_pure_reads_2.fastq'
    params:
        outdir = 'test_pipeline/READ_QC/{sample}'
    log:
        'test_pipeline/logs/metawrap_readqc.{sample}.log'
    threads:
        workflow.cores // len(all_samples)
    container:
        metawrap
    shell:
        'ln -s {input.BMTAGGER_INDEX} ${{HOME}}/BMTAGGER_DB && '
        'mkdir -p {params.outdir} && '
        'metawrap read_qc '
        '-1 {input.r1} '
        '-2 {input.r2} '
        '-t {threads} '
        '-o {params.outdir} '
        '&> {log}'


rule get_fastq:
    input:
        runfile = get_file_url,
        config = 'user-settings.mkfg'
    output:
        r1 = "test_pipeline/read_files/{sample}_1.fastq",
        r2 = "test_pipeline/read_files/{sample}_2.fastq",
    params:
        extra = '--skip-technical --read-filter pass', 
        outdir = 'test_pipeline/read_files/',
        tmpdir = 'test_pipeline/read_files/tmp_{sample}',
        ncbidir = 'output/tmp_ncbi'               # this is /repository/user/default-path
    log:
        'test_pipeline/logs/get_fastq.{sample}.log'
    threads:
        workflow.cores // len(all_samples)
    container:
        sra_tools
    shell:
        # configure the tool
        'mkdir -p {params.ncbidir} || exit 1 ; '   # this is /repository/user/default-path
        'mkdir -p ~/.ncbi || exit 1 ; '            # this is where vdg-config looks
        'cp {input.config} ~/.ncbi/user-settings.mkfg || exit 1 ; '
        # run the download
        'fasterq-dump '
        '--outfile {wildcards.sample} '
        '--outdir {params.outdir} '
        '--temp {params.tmpdir} '
        '--threads {threads} '
        '--details '
        '--split-files '
        '--log-level 5 '
        # '{params.extra} '                        # needs sra-tools 2.11.2
        '{input.runfile} '                         # faster method
        # '{wildcards.run} '                       # slow method
        '&> {log} '